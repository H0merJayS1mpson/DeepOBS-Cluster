# Deepobs for TCML Cluster

Interface for Deeobs on TCML-Cluster

## Getting Started

Copy testobs Folder into your Cluster /home/usr_name folder. Or copy files into folder of your choice.

### Prerequisites

TCML-CLuster Account.

### Running the Interface

### 1.) General Approach

First you will need to generate a Configuration ```.txt``` file. Let's say we call it ```my_configurations.txt```
It has to contain the following:

```
Testprobmlem Name - See Deepobs Documentation for available Testproblems
Optimizer class Name - Name of the optimizer Class
Optimizer Path - Path to where the optimizer is stored
Optimizer module - Module description in Python import style
Set of Hyperparameters for the Optimizer - Hyperparamters which should be tested (every Combination will get tested)
Additional Paramters for Deeobs Trainingphase - See Deepobs Documentation for Details
Sbatch Parameters - Sbatch Parameters used for the configurations
```

### 2.) General info about Entries

Entries of the ```my_configurations.txt``` should generally be structured like this:

```
key: value
```
#### On Hyperparameter values

Given Multiple Hyperparameters the tool will run every possible combination of the given Hyperparamters (cartesian product).
Be aware, that you should look up the correct types for the ```value``` as the wrong types will cause a runtime error.
Every ```value``` will be combined with every other ```value``` which is a Hyperparameter. Resulting in every possible combination of given Hyperparameter values to be run.

### 3.) Values and how they should look like

Every singelton value can be specified as in:

```some key: x``` 

In case of Hyperparameters, values have to be specified in Pyhton dictionary style like this:

```hyperparameters: {"some hyperparameter": x, "another hyperparameter": [x], "yet another hyperparameter"; (x, y ,z)}}``` 

x being of any correct type with respect to the key and the latter, i.e. ```(x, y, z)``` will be interpreted as a range of values. See below.

Non singelton Float, Integer or Boolean values may be specified in the following ways:

#### 1. List- or Array-like

```[x, y, z]``` 

#### 2. Range Notation (Float or Integer only, Hyperparameter only):

```(x, y, z)``` 

representing:

```(lower, upper, increment)``` 

So for example ```(0.1, 0.5, 0.1)``` would result in the Hyperparameter values ```[0.1, 0.2, 0.3, 0.4, 0.5]``` to be run in every 
possible combination with the other Hyperparameters.

### 4.) Examples and Optimizers

#### 4.1.) Using a Pytorch build-in Optimizer

Let's take a look at an example ```my_configurations.txt``` file using pytorch build-in Optimizer ```SGD```
You may use Optimizers coming with pytorch by default like this:

```
Testproblem: mnist_mlp
Optimizer: SGD
hyperparameters: {'lr': 0.01, 'momentum': [0.99], 'nesterov': False}
num_epochs: 1
batch_size: 200
sbatch_job_name: some_experiment_name
sbatch_nnodes: 1
sbatch_ntasks: 1
sbatch_cpus_per_task: 5
sbatch_gres: gpu:1080ti:1
sbatch_partition: test
sbatch_time: 15:00
```

of course ```lr: [0.01]``` or ```nesterov: [False]``` would also be accepted and considered singleton values.

In case of multiple Hyperparameter values the ```my_configurations.txt``` would look something like this:
```
Testproblem: mnist_mlp
Optimizer: SGD
hyperparameters: {'lr': [0.01, 0.02], 'momentum': [0.99, 0.79], 'nesterov': False}
num_epochs: 1
batch_size: 200
sbatch_job_name: some_experiment_name
sbatch_nnodes: 1
sbatch_ntasks: 1
sbatch_cpus_per_task: 5
sbatch_gres: gpu:1080ti:1
sbatch_partition: test
sbatch_time: 15:00
```

In both these cases we have not specified an outputfolder name. Therefore a default ouput folder will be created in the current working directory.
You may also define a output folder name. (See example below on how that works)

*** IMPORTANT NOTE ***

In this folder only the **output** generated by deepobs will be saved. Along with possible error messages generated in the process of the calculation.
The results will by standard be saved in the **results** folder generated by deepobs in your current working directory.
(As you may of course call the tool via  ``` python3 path_to_testobs/testobs path_to_configurations_file/my_configurations.txt ```)

#### 4.2.) Using custom (user written) Optimizer

We could also specify a user written Optimizer which would have to be a subclass of the pytorch optimizer class (See Deeobs Documentation for further Info)
In this case you would have to specify the ```path``` to wherever the Optimizer is located, as well as the ```optimizer class name``` and 
the ```optimizer module```. 
The ```optimizer module``` has to be specified in the way one would specify a Python package import. (See example)

Then the For example your ```my_configurations.txt``` could look like this:

```
Testproblem: mnist_mlp
Optimizer: name_of_optimzer_class
Optimizer Path: /home/usr_name/user_optimizer/user_optimizer_file.py
Optimizer Module: user_optimizer.user_optimizer_file.name_of_optimzer_class
hyperparameters: {'lr': (0.01, 0.05, 0.01), 'momentum': [0.99, 0.79], 'nesterov': False}
num_epochs: 1
batch_size: 200
sbatch_job_name: The_jobs_name
sbatch_nnodes: 1
sbatch_ntasks: 1
sbatch_cpus_per_task: 5
sbatch_gres: gpu:1080ti:1
sbatch_partition: test
sbatch_time: 15:00
output: user_specified_outputfolder

```

## Running the Configurations

After you set up the ```my_configurations.txt``` file as described above simply navigate to the copied folder containing the files and type 

``` python3 slurm_obs_clean.py path_to_configurations_file/my_configurations.txt ```

Of course you do not have to specify the path if the ```my_configurations.txt``` file is located in the same folder.

You may also use a different working directory and launch testobs from there:

``` python3 path_to_testobs_folder/slurm_obs_clean.py path_to_configurations_file/my_configurations.txt ```


## For Infos on Deepobs see:

* [Deepobs Documentation](https://deepobs.readthedocs.io/en/v1.2.0-beta0/) - The DNN Optimizer Benchmark suite
* [Custom Deepobs](https://github.com/H0merJayS1mpson/deepobscustom) - The DNN Optimizer Benchmark suite modified to be able to run the PAL Optimizer as well as allowing to modify the testproblems by altering the weight initializations of the testproblems DNN.


## Acknowledgments

* Hat tip to anyone whose code was used
* etc
